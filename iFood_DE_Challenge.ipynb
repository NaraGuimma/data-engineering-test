{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "iFood_DE_Challenge.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3MziYy-hP8H",
        "outputId": "1f3b33ab-665d-4c4d-824c-7311450af899"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4 MB 62 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 48.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=ea7a23b67caff4ac114862eadafd5a51f77d0169f8baaee56de545e79eac2a29\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/0a/c1/9561f6fecb759579a7d863dcd846daaa95f598744e71b02c77\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkEgKgnqhM8a"
      },
      "source": [
        "import os\n",
        "import pyspark \n",
        "import configparser\n",
        "import pathlib as pl\n",
        "from pyspark.sql import SparkSession"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evsro1ElhJtQ"
      },
      "source": [
        "def create_pyspark_session():\n",
        "\n",
        "  aws_secrets = {\n",
        "    \"PUBLIC_KEY\":\"\",\n",
        "    \"SECRET_KEY\":\"\"\n",
        "  }\n",
        "  config = configparser.ConfigParser()\n",
        "  spark_session = (SparkSession\n",
        "        .builder\n",
        "        .appName('spark_ifood_challenge')\n",
        "        .getOrCreate()\n",
        "      )\n",
        "  # AWS-S3 connection \n",
        "  sc=spark.sparkContext\n",
        "  hadoop_conf=sc._jsc.hadoopConfiguration()\n",
        "  hadoop_conf.set(\"fs.s3n.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n",
        "  hadoop_conf.set(\"fs.s3n.awsAccessKeyId\", aws_secrets.get('PUBLIC_KEY'))\n",
        "  hadoop_conf.set(\"fs.s3n.awsSecretAccessKey\", aws_secrets.get('SECRET_KEY'))\n",
        "\n",
        "  return spark_session"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pN40gBpXL9ih"
      },
      "source": [
        "def ingest_data(s3_paths, spark:SparkSession):\n",
        "  for s3_path in s3_paths:\n",
        "\n",
        "      output_path = \"output_path\"\n",
        "\n",
        "      if s3_path.endswith('csv.gz'):\n",
        "          df = spark.read.csv(s3_path, header=True)\n",
        "      else:\n",
        "          continue\n",
        "\n",
        "      (df.write.format(\"parquet\").mode(write_mode).save(output_path))\n",
        "      \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfQXKHNZhZv4"
      },
      "source": [
        "def extract_data(spark:SparkSession):\n",
        "  final_df = (spark \\\n",
        "              .read \\\n",
        "              .format('parquet') \\\n",
        "              .load(self.parquet_filenames[param_filename])\n",
        "            )\n",
        "  \n",
        "  '''\n",
        "  Add any necessary transformation, if any, with .withColumn\n",
        "  if we want to load the data into snowflake, after mapping the respective columns, we could use:\n",
        "    self.snowflake_connection = {\n",
        "            \"sfUrl\": 'url',\n",
        "            \"sfUser\": 'user',\n",
        "            \"sfPassword\": 'password',\n",
        "            \"sfDatabase\": 'database',\n",
        "            \"sfSchema\": 'schema',\n",
        "            \"sfWarehouse\": 'warehouse'\n",
        "          }\n",
        "\n",
        "    (final_df\\\n",
        "            .write\\\n",
        "            .format('snowflake')\\\n",
        "            .option('dbtable', target_table_name)\\\n",
        "            .options(**snowflake_connection)\\\n",
        "            .option('column_mapping', 'name')\\\n",
        "            .option('truncate_table', 'on')\\\n",
        "            .option('usestagingtable', 'off')\\\n",
        "            .option('columnmap', column_map) \\\n",
        "            .mode('overwrite')\\\n",
        "            .save()    \n",
        "        )\n",
        "\n",
        "  '''\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-qk6RBSgV_T"
      },
      "source": [
        "files_to_ingest = ['restaurant.csv.gz', 'consumer.csv.gz']\n",
        "s3_paths = [f's3n://ifood-data-architect-test-source/{f}' for f in files_to_ingest]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCwdebbcizyT"
      },
      "source": [
        "FOR THE FILE: restaurant.csv.gz"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkRcyxtsf331"
      },
      "source": [
        "spark = create_pyspark_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTXNUxmIgWbZ"
      },
      "source": [
        "ingest_data('s3n://ifood-data-architect-test-source/restaurant.csv.gz', spark)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ke5QZzsphN-b"
      },
      "source": [
        "extract_data(spark)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WUg6QpGi3HS"
      },
      "source": [
        "FOR THE FILE: consumer.csv.gz"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLq0ifHKu-uz"
      },
      "source": [
        "spark = create_pyspark_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0CpeYaCvDXZ"
      },
      "source": [
        "ingest_data('s3n://ifood-data-architect-test-source/consumer.csv.gz', spark)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rK1QPm5CvHve"
      },
      "source": [
        "extract_data(spark)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}